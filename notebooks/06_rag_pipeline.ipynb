{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c80e881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "DATA_PROCESSED_PATH = \"../data/processed\"\n",
    "FINETUNED_RETRIEVER_PATH = \"../models/retriever_finetuned/best-model\"\n",
    "LOCAL_GENERATOR_PATH = \"../models/generator_qwen\"\n",
    "\n",
    "WIKIPEDIA_CHUNKS_FILE = \"wikipedia_chunks.jsonl\"\n",
    "CORPUS_EMBEDDINGS_FILE = \"corpus_embeddings_finetuned.npy\"\n",
    "\n",
    "USE_GENERATOR_QUANTIZATION_ON_LOAD = True\n",
    "GENERATOR_QUANTIZATION_TYPE_ON_LOAD = \"int8\"\n",
    "\n",
    "TOP_K_RETRIEVER = 3\n",
    "MAX_CONTEXT_TOKENS_FOR_GENERATOR = 2048\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "wikipedia_chunks_path = os.path.join(DATA_PROCESSED_PATH, WIKIPEDIA_CHUNKS_FILE)\n",
    "corpus_embeddings_path = os.path.join(DATA_PROCESSED_PATH, CORPUS_EMBEDDINGS_FILE)\n",
    "\n",
    "if not os.path.exists(FINETUNED_RETRIEVER_PATH):\n",
    "    raise FileNotFoundError(f\"Fine-tuned retriver not found: {FINETUNED_RETRIEVER_PATH}\")\n",
    "if not os.path.exists(LOCAL_GENERATOR_PATH):\n",
    "    raise FileNotFoundError(f\"Generator not found locally: {LOCAL_GENERATOR_PATH}\")\n",
    "if not os.path.exists(wikipedia_chunks_path):\n",
    "    raise FileNotFoundError(f\"Wikipedia corpus file not found: {wikipedia_chunks_path}\")\n",
    "if not os.path.exists(corpus_embeddings_path):\n",
    "    raise FileNotFoundError(f\"Corpus embeddings file not found: {corpus_embeddings_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a42e883e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever loaded.\n",
      "Loaded 36508 passages.\n",
      "Corpus embeddings loaded. Shape: (36508, 768)\n"
     ]
    }
   ],
   "source": [
    "retriever_model = SentenceTransformer(FINETUNED_RETRIEVER_PATH, device=DEVICE)\n",
    "print(\"Retriever loaded.\")\n",
    "\n",
    "corpus_passages_data = []\n",
    "with open(wikipedia_chunks_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        corpus_passages_data.append(json.loads(line))\n",
    "print(f\"Loaded {len(corpus_passages_data)} passages.\")\n",
    "\n",
    "corpus_embeddings = np.load(corpus_embeddings_path)\n",
    "print(f\"Corpus embeddings loaded. Shape: {corpus_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eda11298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator's tokenizer loaded from local path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mbagnows/studia/projekty/sem6/rag-wiki/.venv/lib/python3.12/site-packages/transformers/quantizers/auto.py:222: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator loaded from local path.\n"
     ]
    }
   ],
   "source": [
    "generator_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    LOCAL_GENERATOR_PATH, use_fast=True, trust_remote_code=True\n",
    ")\n",
    "if generator_tokenizer.pad_token_id is None:\n",
    "    generator_tokenizer.pad_token_id = generator_tokenizer.eos_token_id\n",
    "print(\"Generator's tokenizer loaded from local path.\")\n",
    "\n",
    "quantization_config_gen = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model_gen_kwargs = {\"device_map\": \"auto\", \"trust_remote_code\": True}\n",
    "model_gen_kwargs[\"quantization_config\"] = quantization_config_gen\n",
    "\n",
    "generator_model = AutoModelForCausalLM.from_pretrained(LOCAL_GENERATOR_PATH, **model_gen_kwargs)\n",
    "print(\"Generator loaded from local path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cd27ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_contexts(query_text, top_k=TOP_K_RETRIEVER):\n",
    "    query_embedding = retriever_model.encode(query_text, convert_to_numpy=True)\n",
    "    if query_embedding.ndim == 1:\n",
    "        query_embedding = query_embedding.reshape(1, -1)\n",
    "\n",
    "    cosine_scores = util.cos_sim(query_embedding, corpus_embeddings)[0].numpy()\n",
    "\n",
    "    if top_k >= len(cosine_scores):\n",
    "        top_k_indices = np.argsort(cosine_scores)[::-1][:top_k]\n",
    "    else:\n",
    "        top_k_indices_unsorted = np.argpartition(-cosine_scores, range(top_k))[:top_k]\n",
    "        top_k_indices = top_k_indices_unsorted[np.argsort(-cosine_scores[top_k_indices_unsorted])]\n",
    "\n",
    "    retrieved_passages = [corpus_passages_data[idx] for idx in top_k_indices]\n",
    "    return retrieved_passages, [float(cosine_scores[idx]) for idx in top_k_indices]\n",
    "\n",
    "\n",
    "def format_rag_prompt(query_text, retrieved_contexts_data):\n",
    "    context_str = \"\"\n",
    "    for i, context_data in enumerate(retrieved_contexts_data):\n",
    "        context_str += f\"Context [{i+1}]: {context_data['passage_text']}\\n\\n\"\n",
    "\n",
    "    instruction = (\n",
    "        \"Answer the given question USING ONLY information in provided contexts.\"\n",
    "        \"If the exact answer doesn't apper in provided context, then EXACTLY THIS: \"\n",
    "        \"'Sorry, I don't know the answer based on the articles provided.'\"\n",
    "        \"and don't say anything after that\\n\\n\"\n",
    "    )\n",
    "\n",
    "    full_prompt_content = f\"{instruction}Provided context:\\n{context_str}Question: {query_text}\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": full_prompt_content}]\n",
    "    formatted_prompt = generator_tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return formatted_prompt, full_prompt_content\n",
    "\n",
    "\n",
    "def generate_answer_with_rag(\n",
    "    query_text,\n",
    "    max_new_tokens=250,\n",
    "    temperature=0.1,\n",
    "    top_p=0.9,\n",
    "    do_sample=False,\n",
    "    print_passages=True,\n",
    "    print_details=True,\n",
    "):\n",
    "    retrieved_contexts, scores = retrieve_contexts(query_text, top_k=TOP_K_RETRIEVER)\n",
    "    if not retrieved_contexts:\n",
    "        return \"Sorry, I failed to find any articles to answer this question.\"\n",
    "\n",
    "    if print_passages:\n",
    "        print(\"Retrived passages:\")\n",
    "        for i, (ctx, score) in enumerate(zip(retrieved_contexts, scores)):\n",
    "            if print_details:\n",
    "                print(\n",
    "                    f\"  (Rank: {i+1}) (PassageID: {ctx['passage_id']}) (Score: {score:.4f}) (Title: {ctx['document_title']}) \\\"{ctx['passage_text']}\\\"\"\n",
    "                )\n",
    "            else:\n",
    "                print(f\"  {ctx['passage_text']}\")\n",
    "\n",
    "    rag_prompt_formatted, _ = format_rag_prompt(query_text, retrieved_contexts)\n",
    "    inputs = generator_tokenizer(\n",
    "        rag_prompt_formatted, return_tensors=\"pt\", padding=False, truncation=False\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = generator_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature if do_sample else 1.0,\n",
    "            top_p=top_p if do_sample else 1.0,\n",
    "            do_sample=do_sample,\n",
    "            pad_token_id=generator_tokenizer.pad_token_id,\n",
    "            eos_token_id=generator_tokenizer.eos_token_id,\n",
    "        )\n",
    "\n",
    "    response_text = generator_tokenizer.decode(\n",
    "        outputs[0][inputs.input_ids.shape[-1] :], skip_special_tokens=True\n",
    "    )\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ccf04db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: what is the title of the second harry potter book\n",
      "\n",
      "Retrived passages:\n",
      "  The second book, Harry Potter and the Chamber of Secrets, was originally published in the UK on 2 July 1998 and in the US on 2 June 1999. Harry Potter and the Prisoner of Azkaban was published a year later in the UK on 8 July 1999 and in the US on 8 September 1999. Harry Potter and the Goblet of Fire was published on 8 July 2000 at the same time by Bloomsbury and Scholastic. Harry Potter and the Order of the Phoenix is the longest book in the series, at 766 pages in the UK version and 870 pages in the US version. It was published worldwide in English on 21 June 2003. Harry Potter and the Half-Blood Prince was published on 16 July 2005. The seventh and final novel, Harry Potter and the Deathly Hallows, was published on 21 July 2007. Rowling herself has stated that the last chapter of the final book (in fact, the epilogue) was completed \"in something like 1990\". Rowling retained rights to digital editions and released them on the Pottermore website in 2012. Vendors such as Amazon displayed the ebooks in the form of links to Pottermore, which controlled pricing. All seven Harry Potter novels have been released in unabridged audiobook versions, with Stephen Fry reading the British editions and Jim Dale voicing the series for the American editions. On Audible, the series has been listened, as of November 2022, for over a billion hours.\n",
      "  == Legacy == === Sequels === The second book, Harry Potter and the Chamber of Secrets, was originally published in the UK on 2 July 1998 and later, in the US on 2 June 1999. Harry Potter and the Prisoner of Azkaban was then published a year later in the UK on 8 July 1999 and in the US on 8 September 1999. Harry Potter and the Goblet of Fire was published on 8 July 2000 at the same time by Bloomsbury and Scholastic. Harry Potter and the Order of the Phoenix is the longest book in the series at 766 pages in the UK version and 870 pages in the US version. It was published worldwide in English on 21 June 2003. Harry Potter and the Half-Blood Prince was published on 16 July 2005 and sold 11 million copies in the first 24 hours of its worldwide release. The seventh and final novel, Harry Potter and the Deathly Hallows, was published on 21 July 2007. The book sold 11 million copies within 24 hours of its release: 2.7 million copies in the UK and 8.3 million in the US. === Illustrated version === An illustrated version of Harry Potter and the Philosopher's Stone was released on 6 October 2015, with illustrations by Jim Kay. The book carries over 100 illustrations and will be followed by illustrated versions of all seven books from the series by the same artist.\n",
      "  == Background == === Series === The first novel in the Harry Potter series, Harry Potter and the Philosopher's Stone, was published by Bloomsbury in 1997. It was followed by Chamber of Secrets (1998), Prisoner of Azkaban (1999), Goblet of Fire (2000), Order of the Phoenix (2003) and Half-Blood Prince (2005). === Title === The title of the novel refers to three mythical objects featured in the story, which are collectively known as the \"Deathly Hallows\". Rowling announced the title in December 2006 through a Christmas-themed hangman puzzle on her website. Other titles that Rowling considered were Harry Potter and the Elder Wand and Harry Potter and the Peverell Quest. === Writing ===\n",
      "\n",
      "Response: The title of the second Harry Potter book is Harry Potter and the Chamber of Secrets.\n"
     ]
    }
   ],
   "source": [
    "test_query_rag1 = \"what is the title of the second harry potter book\"\n",
    "print(f\"Question: {test_query_rag1}\\n\")\n",
    "res = generate_answer_with_rag(test_query_rag1, print_details=False)\n",
    "print(f\"\\nResponse: {res}\")\n",
    "\n",
    "# print(\"\\n\" + \"#\"*70 + \"\\n\")\n",
    "\n",
    "# test_query_rag2 = \"What is the main export product of Japan according to the provided texts?\"\n",
    "# print(f\"Question: {test_query_rag2}\\n\")\n",
    "# res = generate_answer_with_rag(test_query_rag2)\n",
    "# print(f\"\\nResponse: {res}\")\n",
    "\n",
    "# print(\"\\n\" + \"#\"*70 + \"\\n\")\n",
    "\n",
    "# test_query_rag3 = \"Who is the current president of Mars based on the documents?\" # question with no anwser in wiki corpus\n",
    "# print(f\"Question: {test_query_rag3}\\n\")\n",
    "# res = generate_answer_with_rag(test_query_rag3, print_details=False)\n",
    "# print(f\"\\nResponse: {res}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
