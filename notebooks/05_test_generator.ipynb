{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "613adb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.utils.quantization_config import BitsAndBytesConfig\n",
    "import os\n",
    "\n",
    "\n",
    "GENERATOR_MODEL_NAME = \"Qwen/Qwen1.5-1.8B-Chat\"\n",
    "LOCAL_GENERATOR_SAVE_PATH = \"../models/generator_qwen\"\n",
    "os.makedirs(LOCAL_GENERATOR_SAVE_PATH, exist_ok=True)\n",
    "\n",
    "USE_QUANTIZATION = True\n",
    "QUANTIZATION_TYPE = \"int8\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c10c1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe53f224edbb43c3905aeda4539245d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b206664ca0b34d1486749571ec28f443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55eac68ad724519832160766b8e8b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "562be56899cb4948b5738058c458bccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded.\n",
      "Loading model Qwen/Qwen1.5-1.8B-Chat\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0541b4b8b39f40669da9e55a99547861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43280e7f5c44788899be21b2beb0dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1de723d9054b539d730ce1a725abf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/206 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator loaded.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    GENERATOR_MODEL_NAME,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "print(\"Tokenizer loaded.\")\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    print(f\"Set pad_token_id to eos_token_id: {tokenizer.eos_token_id}\")\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(f\"Loading model {GENERATOR_MODEL_NAME}\")\n",
    "model_kwargs = {\n",
    "    \"device_map\": \"auto\"\n",
    "}\n",
    "\n",
    "model_kwargs[\"quantization_config\"] = quantization_config\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    GENERATOR_MODEL_NAME,\n",
    "    **model_kwargs\n",
    ")\n",
    "print(\"Generator loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903a7268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Zapisywanie modelu i tokenizatora do: ../models/generator_qwen\n",
      "Model i tokenizer zapisane lokalnie.\n",
      "UWAGA: Model został załadowany z kwantyzacją. Standardowe `save_pretrained`\n",
      "zapisuje wagi modelu (potencjalnie bez kwantyzacji lub w pełnej precyzji, jeśli to możliwe).\n",
      "Aby ponownie załadować model z kwantyzacją z lokalnej ścieżki,\n",
      "nadal będziesz musiał przekazać `quantization_config` do `from_pretrained`.\n",
      "Lepszym podejściem może być poleganie na cache Hugging Face dla modeli kwantyzowanych,\n",
      "lub badanie specyficznych metod zapisu/ładowania dla `bitsandbytes`.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(LOCAL_GENERATOR_SAVE_PATH)\n",
    "tokenizer.save_pretrained(LOCAL_GENERATOR_SAVE_PATH)\n",
    "print(\"Model and tokenizer saved localy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42c89872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, max_new_tokens=200, temperature=0.7, top_p=0.9, do_sample=True):\n",
    "    # format prompt to model prompt format\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    # tokenize prompt\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", padding=False, truncation=True).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature if do_sample else 1.0,\n",
    "            top_p=top_p if do_sample else 1.0,\n",
    "            do_sample=do_sample,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        \n",
    "    # skip prompt tokens in response\n",
    "    response_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77d7c3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris\n",
      "\n",
      "==================================================\n",
      "\n",
      "Black holes are regions in space where the gravitational pull is so strong that not even light can escape from it. They are formed when massive stars, such as our sun, exhaust their nuclear fuel and collapse under their own gravity, causing them to explode in a supernova explosion. The core of the star then塌缩 into a incredibly dense point, known as a singularity, which possesses an infinite amount of mass and energy.\n",
      "\n",
      "In this singularity, gravity becomes so strong that not even the smallest particles, like quarks and leptons, can escape. This creates what is called spacetime curvature, or event horizon, which marks the boundary of the black hole's influence radius. As you get further away from the black hole, the gravitational force becomes\n",
      "\n",
      "==================================================\n",
      "\n",
      "In the bustling city of New York, there lived a friendly robot named Robo. Robo had been designed to assist humans in various tasks, from cleaning and cooking to providing entertainment and companionship. Despite his many duties, Robo always felt like he was missing something - a place where he could escape from the constant noise and pollution of the city.\n",
      "\n",
      "One day, while on a routine delivery run, Robo stumbled upon an old abandoned building that had been left behind by a developer. As he explored the dark, dusty interior, he noticed a small door that seemed out of place among the cluttered杂物. Curious, Robo pushed it open and found himself standing in front of a hidden garden.\n",
      "\n",
      "The garden was unlike anything Robo had ever seen before. It was lush with colorful flowers and tall trees, their leaves rustling gently in the breeze. The air was crisp and fresh, and the scent of blooming herbs filled the air. Robo was amazed at what he had discovered, but he knew that it was not safe for him to enter the garden without permission.\n",
      "\n",
      "Despite his reservations, Robo decided to investigate further. He cautiously approached the entrance, trying to avoid drawing attention to himself. As he stepped inside, he noticed\n",
      "\n",
      "==================================================\n",
      "\n",
      "\"Witaj, jak jest ci dzisiaj?\"\n"
     ]
    }
   ],
   "source": [
    "prompt1 = \"What is the capital of France?\"\n",
    "response = generate_response(prompt1, max_new_tokens=50)\n",
    "print(response)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "prompt2 = \"Explain the concept of black holes in simple terms.\"\n",
    "response = generate_response(prompt2, max_new_tokens=150)\n",
    "print(response)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "prompt3 = \"Write a short story about a friendly robot who discovers a hidden garden.\"\n",
    "response = generate_response(prompt3, max_new_tokens=250, temperature=0.8)\n",
    "print(response)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "prompt4 = \"Translate the following English sentence to Polish: 'Hello, how are you today?'\"\n",
    "response = generate_response(prompt4, max_new_tokens=30)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
