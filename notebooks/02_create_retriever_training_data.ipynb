{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd14f84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "import torch\n",
    "\n",
    "DATA_INTERIM_PATH = \"../data/interim\"\n",
    "DATA_PROCESSED_PATH = \"../data/processed\"\n",
    "NQ_QUESTIONS_FILE = \"nq_questions_1000.jsonl\"\n",
    "WIKIPEDIA_CHUNKS_FILE = \"wikipedia_chunks_bge_base.jsonl\"\n",
    "OUTPUT_TRAINING_FILE = \"retriever_training_data.jsonl\"\n",
    "\n",
    "\n",
    "MODEL_TEACHER_NAME = \"BAAI/bge-large-en-v1.5\"\n",
    "K_QUERY = 50\n",
    "THRESHOLD_ANSWER_SIMILARITY = 0.5\n",
    "MIN_THRESHOLD_ANSWER_SIMILARITY = 0.25\n",
    "NUM_POSITIVES_PER_QUERY = 2\n",
    "NUM_HARD_NEGATIVES_PER_QUERY = 5\n",
    "NUM_EASY_NEGATIVES_PER_QUERY = 1\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "nq_questions_path = os.path.join(DATA_INTERIM_PATH, NQ_QUESTIONS_FILE)\n",
    "wikipedia_chunks_path = os.path.join(DATA_INTERIM_PATH, WIKIPEDIA_CHUNKS_FILE)\n",
    "output_training_path = os.path.join(DATA_PROCESSED_PATH, OUTPUT_TRAINING_FILE)\n",
    "\n",
    "if not os.path.exists(nq_questions_path):\n",
    "    raise FileNotFoundError(f\"NQ file not found: {nq_questions_path}\")\n",
    "if not os.path.exists(wikipedia_chunks_path):\n",
    "    raise FileNotFoundError(f\"Wiki corupus file not found: {wikipedia_chunks_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35856d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 NQ quesions with answers\n",
      "Loaded 36508 Wikipedia chunks.\n"
     ]
    }
   ],
   "source": [
    "nq_data = []\n",
    "with open(nq_questions_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        record = json.loads(line)\n",
    "        answer_text = record['answer']\n",
    "        if isinstance(answer_text, str) and answer_text.strip():\n",
    "            nq_data.append({\n",
    "                \"id\": record.get(\"id\", f\"nq_unknown_{len(nq_data)}\"),\n",
    "                \"query\": record[\"query\"],\n",
    "                \"answer\": answer_text.strip() \n",
    "            })\n",
    "\n",
    "\n",
    "print(f\"Loaded {len(nq_data)} NQ quesions with answers\")\n",
    "if not nq_data:\n",
    "    raise ValueError(\"No NQ question loaded\")\n",
    "\n",
    "passages_data = []\n",
    "passage_texts_for_embedding = []\n",
    "passage_id_map = {}\n",
    "\n",
    "with open(wikipedia_chunks_path, 'r', encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        record = json.loads(line)\n",
    "        if 'passage_id' in record and 'passage_text' in record and record['passage_text'].strip():\n",
    "            passages_data.append(record)\n",
    "            passage_texts_for_embedding.append(record['passage_text'])\n",
    "            passage_id_map[i] = record['passage_id']\n",
    "\n",
    "\n",
    "print(f\"Loaded {len(passages_data)} Wikipedia chunks.\")\n",
    "if not passages_data:\n",
    "    raise ValueError(\"No Wikipedia chunks loaded\")\n",
    "\n",
    "query_texts_for_embedding = [item['query'] for item in nq_data]\n",
    "answer_texts_for_embedding = [item['answer'] for item in nq_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c25f29a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded teacher retriver: BAAI/bge-large-en-v1.5\n",
      "Generating embeddings for NQ questions\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "028e58a7be70478c9422cb20f2cade3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions embeddins shape: torch.Size([1000, 1024])\n",
      "Generating embeddings for NQ answers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc213a3c1a0b492ba320d25e96e42d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anwsers embeddins shape: torch.Size([1000, 1024])\n",
      "Generating embeddings for Wikipedia chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d682e07669574b71bba1865540f9d83b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1141 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia chunks embeddins shape: torch.Size([36508, 1024])\n"
     ]
    }
   ],
   "source": [
    "teacher_model = SentenceTransformer(MODEL_TEACHER_NAME, device=DEVICE)\n",
    "print(f\"Loaded teacher retriver: {MODEL_TEACHER_NAME}\")\n",
    "\n",
    "print(\"Generating embeddings for NQ questions\")\n",
    "query_embeddings = teacher_model.encode(query_texts_for_embedding, convert_to_tensor=True, show_progress_bar=True)\n",
    "print(f\"Questions embeddins shape: {query_embeddings.shape}\")\n",
    "\n",
    "print(\"Generating embeddings for NQ answers\")\n",
    "answer_embeddings = teacher_model.encode(answer_texts_for_embedding, convert_to_tensor=True, show_progress_bar=True)\n",
    "print(f\"Anwsers embeddins shape: {answer_embeddings.shape}\")\n",
    "\n",
    "print(\"Generating embeddings for Wikipedia chunks...\")\n",
    "passage_embeddings = teacher_model.encode(passage_texts_for_embedding, convert_to_tensor=True, show_progress_bar=True)\n",
    "print(f\"Wikipedia chunks embeddins shape: {passage_embeddings.shape}\")\n",
    "\n",
    "if DEVICE == \"cuda\":\n",
    "    query_embeddings = query_embeddings.cpu()\n",
    "    answer_embeddings = answer_embeddings.cpu()\n",
    "    passage_embeddings = passage_embeddings.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e97367d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aca003985cb4858ad459111bcb60481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing NQ questions:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Created 5604 training indicies.\n",
      "Class distribution: Positive (1): 1998, Negative (0): 3606\n"
     ]
    }
   ],
   "source": [
    "retriever_training_samples = []\n",
    "processed_passage_ids_for_query = {} # to avoid duplicates\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(nq_data)), desc=\"Processing NQ questions\"):\n",
    "    current_query_data = nq_data[i]\n",
    "    current_query_text = current_query_data['query']\n",
    "    current_query_embedding = query_embeddings[i]\n",
    "    current_answer_embedding = answer_embeddings[i]\n",
    "    \n",
    "    query_id = current_query_data['id']\n",
    "    processed_passage_ids_for_query[query_id] = set()\n",
    "\n",
    "    # similarity between question and all documents\n",
    "    query_passage_similarities = cos_sim(current_query_embedding, passage_embeddings)[0]\n",
    "    \n",
    "    # sort by similarity\n",
    "    try:\n",
    "        query_passage_similarities_np = query_passage_similarities.numpy()\n",
    "    except AttributeError: # Jeśli już jest NumPy array\n",
    "        query_passage_similarities_np = query_passage_similarities\n",
    "\n",
    "    # take indexes of the best passages\n",
    "    # [::-1] reverse the order (descending)\n",
    "    top_k_query_indices = np.argsort(query_passage_similarities_np)[::-1][:K_QUERY]\n",
    "\n",
    "    positive_candidates = []\n",
    "    hard_negative_candidates_from_query_ranking = []\n",
    "\n",
    "    # evaluation of these passages by comparing their embeddings to anwsers\n",
    "    for passage_idx in top_k_query_indices:\n",
    "        passage_id = passage_id_map[passage_idx]\n",
    "        passage_text = passage_texts_for_embedding[passage_idx]\n",
    "        passage_embedding = passage_embeddings[passage_idx]\n",
    "        \n",
    "        sim_to_query = float(query_passage_similarities_np[passage_idx])\n",
    "        sim_to_answer = float(cos_sim(passage_embedding, current_answer_embedding)[0][0])\n",
    "\n",
    "        candidate_info = {\n",
    "            \"query_id\": query_id,\n",
    "            \"query_text\": current_query_text,\n",
    "            \"passage_id\": passage_id,\n",
    "            \"passage_text\": passage_text,\n",
    "            \"sim_to_query\": sim_to_query,\n",
    "            \"sim_to_answer\": sim_to_answer\n",
    "        }\n",
    "\n",
    "        if sim_to_answer >= THRESHOLD_ANSWER_SIMILARITY:\n",
    "            positive_candidates.append(candidate_info)\n",
    "        else:\n",
    "            hard_negative_candidates_from_query_ranking.append(candidate_info)\n",
    "    \n",
    "    # emergency loop with lower similarity requirement if none positive candidates were found\n",
    "    if len(positive_candidates) == 0:\n",
    "        for passage_idx in top_k_query_indices:\n",
    "            passage_id = passage_id_map[passage_idx]\n",
    "            passage_text = passage_texts_for_embedding[passage_idx]\n",
    "            passage_embedding = passage_embeddings[passage_idx]\n",
    "            \n",
    "            sim_to_query = float(query_passage_similarities_np[passage_idx])\n",
    "            sim_to_answer = float(cos_sim(passage_embedding, current_answer_embedding)[0][0])\n",
    "\n",
    "            candidate_info = {\n",
    "                \"query_id\": query_id,\n",
    "                \"query_text\": current_query_text,\n",
    "                \"passage_id\": passage_id,\n",
    "                \"passage_text\": passage_text,\n",
    "                \"sim_to_query\": sim_to_query,\n",
    "                \"sim_to_answer\": sim_to_answer\n",
    "            }\n",
    "\n",
    "            if sim_to_answer >= MIN_THRESHOLD_ANSWER_SIMILARITY:\n",
    "                positive_candidates.append(candidate_info)\n",
    "            \n",
    "            \n",
    "    # sort positives by similarity to answer\n",
    "    positive_candidates.sort(key=lambda x: x['sim_to_answer'], reverse=True)\n",
    "    \n",
    "    num_added_positives = 0\n",
    "    for cand in positive_candidates:\n",
    "        if num_added_positives < NUM_POSITIVES_PER_QUERY and cand['passage_id'] not in processed_passage_ids_for_query[query_id]:\n",
    "            retriever_training_samples.append({\n",
    "                \"query\": cand['query_text'], \n",
    "                \"passage\": cand['passage_text'], \n",
    "                \"label\": 1\n",
    "            })\n",
    "            processed_passage_ids_for_query[query_id].add(cand['passage_id'])\n",
    "            num_added_positives += 1\n",
    "\n",
    "\n",
    "    # sort hard negatives by those highly similar to question but not similar to answer (descending)\n",
    "    hard_negative_candidates_from_query_ranking.sort(key=lambda x: x['sim_to_query'] - x['sim_to_answer'], reverse=True)\n",
    "    \n",
    "    num_added_hard_negatives = 0\n",
    "    for cand in hard_negative_candidates_from_query_ranking:\n",
    "        if num_added_hard_negatives < NUM_HARD_NEGATIVES_PER_QUERY and cand['passage_id'] not in processed_passage_ids_for_query[query_id]:\n",
    "            retriever_training_samples.append({\n",
    "                \"query\": cand['query_text'], \n",
    "                \"passage\": cand['passage_text'], \n",
    "                \"label\": 0\n",
    "            })\n",
    "            processed_passage_ids_for_query[query_id].add(cand['passage_id'])\n",
    "            num_added_hard_negatives += 1\n",
    "            \n",
    "    # add easy negatives\n",
    "    num_added_easy_negatives = 0\n",
    "    attempts_easy_neg = 0\n",
    "    all_passage_indices = list(range(len(passages_data)))\n",
    "    potential_easy_negative_indices = [idx for idx in all_passage_indices if idx not in top_k_query_indices]\n",
    "    \n",
    "    if potential_easy_negative_indices:\n",
    "        while num_added_easy_negatives < NUM_EASY_NEGATIVES_PER_QUERY and attempts_easy_neg < (NUM_EASY_NEGATIVES_PER_QUERY * 10):\n",
    "            attempts_easy_neg += 1\n",
    "            if not potential_easy_negative_indices: break\n",
    "\n",
    "            random_passage_idx = random.choice(potential_easy_negative_indices)\n",
    "            random_passage_id = passage_id_map[random_passage_idx]\n",
    "            \n",
    "            if random_passage_id not in processed_passage_ids_for_query[query_id]:\n",
    "                random_passage_text = passage_texts_for_embedding[random_passage_idx]\n",
    "                retriever_training_samples.append({\n",
    "                    \"query\": current_query_text, \n",
    "                    \"passage\": random_passage_text, \n",
    "                    \"label\": 0\n",
    "                })\n",
    "                processed_passage_ids_for_query[query_id].add(random_passage_id)\n",
    "                num_added_easy_negatives += 1\n",
    "                potential_easy_negative_indices.remove(random_passage_idx) \n",
    "            if not potential_easy_negative_indices: break\n",
    "\n",
    "print(f\"\\nCreated {len(retriever_training_samples)} training indicies.\")\n",
    "if retriever_training_samples:\n",
    "    labels = [sample['label'] for sample in retriever_training_samples]\n",
    "    print(f\"Class distribution: Positive (1): {labels.count(1)}, Negative (0): {labels.count(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78834b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5d59df38834290b490398cb21f2510",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving training dataset:   0%|          | 0/5604 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved retriver's training dataset to: ../data/processed/retriever_training_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "with open(output_training_path, 'w', encoding='utf-8') as f_out:\n",
    "    for sample in tqdm(retriever_training_samples, desc=\"Saving training dataset\"):\n",
    "        f_out.write(json.dumps(sample) + '\\n')\n",
    "print(f\"Saved retriver's training dataset to: {output_training_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
